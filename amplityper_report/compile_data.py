#!/usr/bin/env python3

"""
`compile_data` runs the core Rust functions to perform a potentially large
number of file-system read and write calls, along with commensurate text parsing.
These functions compile information about variants from iVar tables, codon numbers
from snpEff-annotated VCFs, and depths from the `vsearch`-deduplicated FASTA headers.
"""

import itertools
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import amplityper_core as atc  # type: ignore
import polars as pl
from icecream import ic  # type: ignore # pylint: disable=import-error
from result import Err, Ok, Result
from tqdm import tqdm  # type: ignore # pylint: disable=import-error

from .cli import ConfigParams


def construct_file_list(
    results_dir: Path, glob_pattern: Path
) -> Result[List[Path], str]:
    """
        Function `construct_file_list()` uses the provided path of wildcards
        to expand out all available files to be compiled downstream.

    Args:
        `results_dir: Path`: A Pathlib path instance recording the results
        "root" directory, which is the top of the Amplityper results hierarchy.
        `ivar_pattern: Path`: A Pathlib path instance containing wildcards
        that can be expanded to the desired files.

    Returns:
        `Result[List[Path], str]`: A Result type instance containing either
        a list of paths to the desired files, or an error message string.
    """

    ic("Constructing file list:")
    ic(glob_pattern)

    # collect a list of all the files to search
    # pylint: disable-next=c-extension-no-member
    files_to_query = atc.build_file_list(str(results_dir), str(glob_pattern))

    # make sure that there aren't duplicates
    try:
        set(files_to_query)
    except ValueError as message:
        return Err(f"Redundant files present that may corrupt results:\n{message}")

    if len(files_to_query) == 0:
        return Err(
            f"No files found that match the wildcard path:\n{glob_pattern}\nwithin {results_dir}"
        )

    return Ok(files_to_query)


def compile_data_with_io(
    file_list: List[Path], config: ConfigParams, whether_resume: bool
) -> Result[pl.LazyFrame, str]:
    """
        Function `compile_data_with_io()` takes the list of paths and
        reads each file, parsing it with Polars, and writing it into one
        large temporary TSV file. This method for compiling all files into
        one involves a great deal of read-write, but it also avoids potential
        type mismatch issues between a type schema inferred for one dataframe
        and the type schema inferred for the next. Downstream, the new
        TSV can be parsed into a single dataframe where it's possible to
        infer a type scheme from many rows.

    Args:
        `file_list: List[Path]`: A list of paths, where each path points to
        a TSV file generated by iVar.

    Returns:
        `pl.LazyFrame`: A Polars LazyFrame to be queries and transformed
        downstream.
    """

    if os.path.isfile("tmp.arrow") and whether_resume is True:
        all_contigs = pl.scan_ipc("tmp.arrow", memory_map=False)
        os.remove("tmp.arrow")
        return Ok(all_contigs)

    ic("Compiling variant data for each contig.")
    ic(config)

    # Use core module written in Rust to traverse the file system and run
    # read/writes quickly
    atc.collate_results(file_list)  # pylint: disable=c-extension-no-member # pylint: disable=E1101

    ic("Converting variant data to compressed arrow format.")

    # lazily scan the new tmp tsv for usage downstream
    pl.scan_csv("tmp.tsv", separator="\t", infer_schema_length=1500).sort(
        "POS"
    ).sink_ipc("tmp.arrow", compression="zstd")

    all_contigs = pl.scan_ipc("tmp.arrow", memory_map=False)
    os.remove("tmp.tsv")

    return Ok(all_contigs)


def collect_file_lists(
    results_dir: Path, pattern1: Path, pattern2: Path, pattern3: Path
) -> Result[Tuple[List[Path], List[Path], List[Path]], str]:
    """
        Function `collect_file_lists()` quarterbacks sequential executions of
        the `construct_file_list()` function, each with a different wildcard
        pattern. Doing so keeps the size of `main()` more reasonable and will
        also make potential refactoring easier in the future.

    Args:
        `results_dir: Path`: A Pathlib Path type pointing to the results "root"
        directory to be searched with the following glob wildcard patterns.
        `pattern1: Path`: A Pathlib Path type, which can contain wildcards to
        be expanded with the glob library.
        `pattern2: Path`: A Pathlib Path type, which can contain wildcards to
        be expanded with the glob library.
        `pattern3: Path`: A Pathlib Path type, which can contain wildcards to
        be expanded with the glob library.

    Returns:
        `Result[List[List[Path]], str]`: A Result type containing eaither a list of
        lists or an error message string.
    """

    # make a list of the iVar files to query based on the provided wildcard path
    ivar_list_result = construct_file_list(results_dir, pattern1)
    if isinstance(ivar_list_result, Err):
        return Err(
            f"No files found at the provided wildcard path:\n{ivar_list_result.unwrap_err()}"
        )

    # Make a list of FASTA files to pull information from
    fasta_list_result = construct_file_list(results_dir, pattern2)
    if isinstance(fasta_list_result, Err):
        return Err(
            f"No FASTAs found at the provided wildcard path:\n{fasta_list_result.unwrap_err()}"
        )

    # Make a list of "tidy" vcf files to pull codon information from
    tvcf_result = construct_file_list(results_dir, pattern3)
    if isinstance(tvcf_result, Err):
        return Err(
            f"No 'tidy' VCF tables found at the provided wildcard path:\n{tvcf_result.unwrap_err()}"
        )

    return Ok(
        (
            ivar_list_result.unwrap_or([]),
            fasta_list_result.unwrap(),
            tvcf_result.unwrap(),
        )
    )


def _try_parse_int(value: str) -> Optional[int]:
    """
    Helper function that handles the possibility that a read support
    cannot be parsed as an integer from the FASTA defline and returns
    `None` instead of raising an unrecoverable error.
    """
    try:
        return int(value)
    except ValueError:
        return None


def _try_parse_identifier(defline: str) -> Optional[str]:
    """
    Helper function that splits a contig's FASTA defline by the "_" symbol,
    pulls a sample id assuming it is the second item in the underscore-split
    defline, parses out the contig name assuming that string starts with
    "contig", and constructs an amplicon name-sample id-contig identifier
    that will serve as a unique key for each contig across all samples
    and amplicons.
    """

    cleaned_defline = defline.replace(">", "").split(" ")[0]
    items = cleaned_defline.split("_")
    sample_id = items[0]
    (haplotype,) = [item for item in items if "haplotype" in item]
    amplicon = cleaned_defline.replace(sample_id, "").replace(haplotype, "")

    amplicon = amplicon[1:] if amplicon[0] == "_" else amplicon
    amplicon = amplicon[:-1] if amplicon[-1] == "_" else amplicon

    identifier = f"{amplicon}-{sample_id}-{haplotype}"

    return identifier


def _is_valid_utf8(fasta_line: str) -> bool:
    """
    Helper function that double checks that each FASTA defline is valid
    UTF-8 text.
    """
    try:
        fasta_line.encode("utf-8").decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False


def generate_seq_dict(
    input_fasta: List[str], config: ConfigParams
) -> Optional[Dict[Optional[str], Optional[int]]]:
    """
        The function `generate_seq_dict()` uses a series of list comprehensions
        to 1) Test that a FASTA file can be decoded into proper UTF-8 text; 2)
        Parse out the name of the current amplicon; And 3) Parse out a FASTA's
        defline and extract a sample ID and a depth of coverage. It then saves
        these pieces of information into a dictionary, where each key is a
        contig's unique identifier, and each value is the integer depth-of-
        coverage.

    Args:
        `input_fasta: List[str]`: Parsed FASTA lines store as strings in a list.
        `split_char: str`: The character, e.g. "-", to use for splitting the
        FASTA defline to parse out information.

    Returns:
        `Optional[Dict[Optional[str], Optional[int]]]`: An option type, with
        option types being a union of type T or None, that wraps the contig
        identifier-depth dictionary. Each item in the dictionary can also be
        None.
    """

    # make sure the lines can be decoded
    decodable = [_is_valid_utf8(line) for line in input_fasta]
    if False in decodable:
        return None

    deflines = [line for line in input_fasta if line.startswith(">")]
    supports = [
        _try_parse_int(line.split(config.fasta_split_char)[config.fasta_split_index])
        for line in input_fasta
        if line.startswith(">")
    ]
    identifiers = [_try_parse_identifier(defline) for defline in deflines]

    assert len(deflines) == len(
        identifiers
    ), "Mismatch between the number of deflines and number of sequences"

    seq_dict = dict(zip(identifiers, supports))

    return seq_dict


def compile_mutation_codons(
    tvcf_list: List[Path], whether_resume: bool
) -> pl.LazyFrame:
    """
        Function `compile_mutation_codons()` loops through all "tidy" VCF files
        in a provided list of Paths and creates a Polars LazyFrame containing
        1) A column of nucleotide mutation strings, and 2) a column of codon
        numbers for use with amino acid substitutions.

    Args:
        `tvcf_list: List[Path]`: A list containing Pathlib Path types pointing to
        any number of tidy VCF files to be assessed.

    Returns:
        `pl.LazyFrame`: A Polars LazyFrame query that will be evaluated downstream.
    """

    ic("Compiling codon numbers for all coding mutations.")

    if os.path.isfile("tmp.tvcf") and whether_resume is False:
        os.remove("tmp.tvcf")

    if os.path.isfile("tmp.tvcf") and whether_resume is True:
        amassed_codons = pl.read_csv(
            "tmp.tvcf", separator="\t", ignore_errors=True
        ).lazy()
        os.remove("tmp.tvcf")
        return amassed_codons

    header_written = False

    progress_bar = tqdm(total=len(tvcf_list))
    with open("tmp.tvcf", "a", encoding="utf-8") as tmp_file:
        for tidy_vcf in tvcf_list:
            progress_bar.update(1)
            variants = pl.read_csv(tidy_vcf, separator="\t")

            # do a few checks to make sure the loop doesn't hang on unexpected
            # file writes
            if variants.shape[0] == 0:
                continue
            if len(set(variants.columns).intersection({"ref", "pos", "alt"})) != 3:
                continue
            if "info_ANN" not in variants.columns and "NUC_SUB" not in variants.columns:
                continue

            variants = variants.with_columns(
                pl.concat_str(
                    [pl.col("ref"), pl.col("pos"), pl.col("alt")], separator="-"
                ).alias("NUC_SUB")
            ).select(["NUC_SUB", "info_ANN"])

            # separate out nucleotide substitutions and annotations
            nuc_subs = variants.select("NUC_SUB").to_series().to_list()
            anns = variants.select("info_ANN").to_series().to_list()

            # Separate out the tenth annotation, which is the amino acid substitution
            aa_vars = [ann.split("|")[10] for ann in anns]

            # Separate out the codon numbers and make sure they are a numeric type
            codons = ["".join((x for x in codon if x.isdigit())) for codon in aa_vars]
            num_codons = [int(codon) if codon != "" else None for codon in codons]

            # decide whether to write header or just append rows
            if header_written is False:
                write_header = True
                header_written = True
            else:
                write_header = False

            # construct data frame
            codon_df = pl.DataFrame(
                {"NUC_SUB": nuc_subs, "CODON": num_codons}
            ).drop_nulls()

            # write out if any data was found
            if codon_df.shape[0] > 0:
                codon_df.unique().write_csv(
                    tmp_file, separator="\t", include_header=write_header
                )
    progress_bar.close()

    ic("All codons compiled in temporary file.")

    # When the full dataset is ammassed read and "lazify" it
    amassed_codons = pl.read_csv("tmp.tvcf", separator="\t", ignore_errors=True).lazy()
    os.remove("tmp.tvcf")
    return amassed_codons


def compile_contig_depths(fasta_list: List[Path], config: ConfigParams) -> pl.LazyFrame:
    """
        Function `compile_contig_depths()` loops through all FASTA files
        in a provided list of FASTA Paths and creates a Polars LazyFrame containing
        1) A column of unique identifiers for each contig, and 2) a column of read
        supports/depths of coverage for each contig.

    Args:
        `fasta_list: List[Path]`: A list containing Pathlib Path types pointing to
        any number of FASTA files to be assessed.

    Returns:
        `pl.LazyFrame`: A Polars LazyFrame query that will be evaluated downstream.
    """

    ic("Compiling contig depths for each contig FASTA.")

    seq_dicts = []

    progress_bar = tqdm(total=len(fasta_list))
    for fasta in fasta_list:
        progress_bar.update(1)
        with open(fasta, "r", encoding="utf-8") as fasta_contents:
            try:
                fasta_lines = fasta_contents.readlines()
            except UnicodeDecodeError:
                print(
                    f"The FASTA at the following path could not be decoded to utf-8:\n{fasta}"
                )
                continue
            seq_dict = generate_seq_dict(fasta_lines, config)
            if seq_dict is None:
                print(
                    f"The FASTA at the following path could not be decoded to utf-8:\n{fasta}"
                )
                continue
            seq_dicts.append(seq_dict)
    progress_bar.close()

    identifiers = list(
        itertools.chain.from_iterable(seq_dict.keys() for seq_dict in seq_dicts)
    )
    supports = list(
        itertools.chain.from_iterable(seq_dict.values() for seq_dict in seq_dicts)
    )

    depth_df = pl.LazyFrame(
        {"Amplicon-Sample-Contig": identifiers, "Depth of Coverage": supports}
    )

    return depth_df
